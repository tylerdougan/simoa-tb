{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config import (\n",
    "    ALERE_RESULTS_DIR,\n",
    "    CLINICAL_MBV_FILES,\n",
    "    CLINICAL_TEST_FILE,\n",
    "    LOD_COL_FMT,\n",
    "    META_ANALYSIS_FILE,\n",
    "    OUTPUT_DIR,\n",
    "    PROCESSED_DIR,\n",
    ")\n",
    "from utils import Barcode2, LogisticGAM, NestedCV  # noqa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dt(arg):\n",
    "    if pd.isna(arg):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        if isinstance(arg, (int, float)) or (isinstance(arg, str) and \"-\" not in arg):\n",
    "            arg = int(float(arg))\n",
    "            return pd.to_datetime(arg, origin=\"1899-12-30\", unit=\"D\")\n",
    "        return pd.to_datetime(arg)\n",
    "    except pd.errors.OutOfBoundsDatetime:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "clinical_kwargs = {\n",
    "    \"true_values\": [\"HIV+\", \"Positive\", \"Pos\", \"Yes\", \"MTB pos\", \"MTB POS\", \"Detected\"],\n",
    "    \"false_values\": [\n",
    "        \"HIV-\",\n",
    "        \"Negative\",\n",
    "        \"Neg\",\n",
    "        \"No\",\n",
    "        \"MTB neg\",\n",
    "        \"MTB NEG\",\n",
    "        \"Not detected\",\n",
    "        \"MTB pos\",\n",
    "    ],\n",
    "    \"na_values\": [\n",
    "        \"Not done\",\n",
    "        \"Contaminated/lost\",\n",
    "        \"Contaminated/Lost\",\n",
    "        \"Not known\",\n",
    "        \"Don't Know\",\n",
    "        \"Not Applicable\",\n",
    "        \"Not applicable\",\n",
    "        \"Not Done/Not applicable\",\n",
    "        \"Not done/not applicable\",\n",
    "        \"Not done/Not applicable\",\n",
    "        \"Indeterminate  (invalid/error/no result)\",\n",
    "        \"Invalid/error/no result\",\n",
    "        \"Indeterminate\",\n",
    "        \"I-Indeterminate\",\n",
    "        \"No result/Indeterminate\",\n",
    "        \"ND\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Read data files\n",
    "mb_100 = pd.read_excel(\n",
    "    CLINICAL_MBV_FILES[0], sheet_name=\"URINE SAMPLES\", **clinical_kwargs\n",
    ")\n",
    "mb_100[\"Cohort\"] = \"training\"\n",
    "val_320 = pd.read_excel(CLINICAL_MBV_FILES[1], **clinical_kwargs)\n",
    "val_320[\"Cohort\"] = \"validation\"\n",
    "\n",
    "test_244 = pd.read_excel(CLINICAL_TEST_FILE, **clinical_kwargs).rename(\n",
    "    columns={\"age\": \"age of sample (Years)\"}\n",
    ")\n",
    "test_244[\"Cohort\"] = \"test\"\n",
    "\n",
    "mbv = pd.read_csv(PROCESSED_DIR / \"mbv.csv\", index_col=0)\n",
    "val_320.loc[\n",
    "    val_320[\"OS_PatientID\"].isin(\n",
    "        mbv[\"OS_PatientID\"][(mbv[\"Cohort\"] == \"training\") & (mbv[\"p_cat\"] == \"S-C+\")]\n",
    "    ),\n",
    "    \"Cohort\",\n",
    "] = \"training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = (\n",
    "    pd.concat([mb_100, val_320, test_244], ignore_index=True)\n",
    "    .dropna(axis=1, how=\"all\")\n",
    "    .drop(\n",
    "        columns=[\n",
    "            \"SP1_SC_REP_RESULT\",\n",
    "            \"SP2_SC_REP_RESULT\",\n",
    "            \"SP3_LC_REP_RESULT\",\n",
    "            \"OS_Specimen_Type\",\n",
    "            \"OS_Requirement_Name\",\n",
    "            \"SP2_LC_REP_RESULT\",\n",
    "            \"SP_RDST_SP_TYPE\",\n",
    "            \"SP_DST_SP_TYPE\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "dt_cols = clinical.columns[clinical.columns.str.endswith(\"_D\")]\n",
    "for col in dt_cols:\n",
    "    clinical[col] = clinical[col].map(parse_dt)\n",
    "# Replace mistaken dates in TB_TX_HX_START_D and TB_TX_HX_END_D\n",
    "for col in [\"TB_TX_HX_START_D\", \"TB_TX_HX_END_D\"]:\n",
    "    clinical[col] = clinical[col].mask(clinical[col].dt.year < clinical[\"YOB\"])\n",
    "barcode_parsed = clinical[\"barcode\"].map(Barcode2)\n",
    "clinical = pd.concat(\n",
    "    [\n",
    "        clinical,\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Age at Enrol\": clinical[\"ENROL_D\"].dt.year - clinical[\"YOB\"],\n",
    "                \"Barcode Parsed\": barcode_parsed.map(lambda x: x.standard_form()),\n",
    "                \"Barcode Any Aliquot\": barcode_parsed.map(lambda x: x.any_aliquot()),\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Harmonize values\n",
    "RES = [\"SP1_LC_REP_RESULT\", \"QFT_RES\"]\n",
    "clinical[RES] = clinical[RES].replace(\n",
    "    {\"Neg\": False, \"Pos\": True, \"Negative\": False, \"Positive\": True}\n",
    ")\n",
    "XP_RIF = [\"SP1_XP_RIF\", \"SP1_XP_REPEAT_RIF\"]\n",
    "clinical[XP_RIF] = clinical[XP_RIF].replace({\"Not detected\": False, \"Detected\": True})\n",
    "PREDICTIONS = [\n",
    "    \"hsMSD_PairO_Prediction\",\n",
    "    \"hsMSD_PairF__Prediction\",\n",
    "    \"R1 Categorical Prediction\",\n",
    "    \"R2 Categorical Prediction\",\n",
    "]\n",
    "clinical[PREDICTIONS] = clinical[PREDICTIONS].replace(\n",
    "    {\"TB\": True, \"NotTB\": False, \"Non TB\": False}\n",
    ")\n",
    "SMEAR_GRADES = clinical.columns[clinical.columns.str.endswith((\"GRADE\", \"Q\", \"CD4\"))]\n",
    "smear = {\"Scanty\": 0, \"1+\": 1, \"2+\": 2, \"3+\": 3, \"<20\": 0, np.nan: pd.NA}\n",
    "clinical[SMEAR_GRADES] = clinical[SMEAR_GRADES].map(lambda x: smear[x]).convert_dtypes()\n",
    "clinical[\"R1 Numeric Prediction\"] = (\n",
    "    clinical[\"R1 Numeric Prediction\"].replace(\"TMF\", np.inf).astype(float)\n",
    ")\n",
    "clinical[\"FU\"] = clinical[\"FU\"].replace(\n",
    "    {\n",
    "        \"Followed up\": \"Followed Up\",\n",
    "        \"No follow up required\": \"No FU Required\",\n",
    "        \"Lost to follow up\": \"Lost to Follow Up\",\n",
    "    }\n",
    ")\n",
    "clinical[\"CX_DX\"] = clinical[\"CX_DX\"].replace(\n",
    "    {\n",
    "        \"TB likely (pulmonary pleural or pericardial)\": \"TB likely (pulmonary; pleural; pericardial)\",\n",
    "        \"Other Specify\": \"Other\",\n",
    "    }\n",
    ")\n",
    "dsts = clinical.columns[clinical.columns.str.contains(\"DST\")].difference(dt_cols)\n",
    "clinical[dsts] = clinical[dsts].replace({\"S\": \"S-Sensitive\", \"R\": \"R-Resistant\"})\n",
    "clinical = clinical.replace(\"0\", pd.NA)\n",
    "\n",
    "SYMPTOMS = [\n",
    "    \"COUGH\",\n",
    "    \"EXPECTORATION\",\n",
    "    \"HEMOPTYSIS\",\n",
    "    \"CHEST_PAIN\",\n",
    "    \"DYSPNOE\",\n",
    "    \"MALAISE\",\n",
    "    \"FEVER\",\n",
    "    \"SWEATS\",\n",
    "    \"WT_LOSS\",\n",
    "    \"LYMPH_NODE\",\n",
    "]\n",
    "clinical[SYMPTOMS] = clinical[SYMPTOMS].replace({\"NO\": \"No\"})\n",
    "\n",
    "# Ordered categories\n",
    "p_cats = [\n",
    "    \"NonTB_NonLTBI\",\n",
    "    \"NonTB_LTBI\",\n",
    "    \"Likely_subcl_TB\",\n",
    "    \"Clinical_TB\",\n",
    "    \"S-C+\",\n",
    "    \"S+C+\",\n",
    "]\n",
    "ordered_categories = [\n",
    "    (\"p_cat\", p_cats),\n",
    "    (SYMPTOMS, [\"No\", \"Yes\", \"< 2 weeks\", \"< 2 months\", \"= 2 months\", \"â‰¥ 2 months\"]),\n",
    "    (\"Cohort\", [\"training\", \"validation\", \"test\"]),\n",
    "    (\n",
    "        [\"hsMSD_PairO_Level\", \"hsMSD_PairF_Level\"],\n",
    "        [\"No LAM\", \"Low LAM\", \"Middle LAM\", \"High LAM\"],\n",
    "    ),\n",
    "]\n",
    "for cols, cats in ordered_categories:\n",
    "    cat_index = pd.Index(cats, dtype=\"str\")\n",
    "    clinical[cols] = clinical[cols].astype(pd.CategoricalDtype(cat_index, ordered=True))\n",
    "\n",
    "unordered_categories = [\n",
    "    dsts,\n",
    "    clinical.columns[clinical.columns.str.endswith(\"SPC\")],\n",
    "    [\"FU_CXR\", \"FU_SYMPTOMS\", \"2M_FU_SYMPTOMS\", \"4M_FU_SYMPTOMS\"],\n",
    "    [\"FU_TB_TX_STATUS\", \"2M_TB_TX_STATUS\"],\n",
    "    [\"Country\"],\n",
    "    [\"SEX\"],\n",
    "    [\"FU\"],\n",
    "    [\"CX_DX\"],\n",
    "    [\"STUDY_NAME\"],\n",
    "    [\"SP1_XP_REPEAT_RESULT\"],\n",
    "    [\"TB_TX_ENROL_SCHEME\"],\n",
    "    [\"FU_TB_DIAG\"],\n",
    "]\n",
    "for cols in unordered_categories:\n",
    "    cat_index = pd.Index(pd.unique(clinical[cols].values.ravel())).dropna()\n",
    "    clinical[cols] = clinical[cols].astype(pd.CategoricalDtype(cat_index))\n",
    "\n",
    "clinical[\"PID_f\"] = clinical[\"PID_b\"] = clinical[\"OS_PatientID\"]\n",
    "clinical = (\n",
    "    clinical.groupby(\"PID_f\")\n",
    "    .ffill()\n",
    "    .groupby(\"PID_b\")\n",
    "    .bfill()\n",
    "    .drop_duplicates(subset=\"OS_PatientID\")\n",
    ")\n",
    "clinical.index = clinical[\"Barcode Any Aliquot\"]\n",
    "\n",
    "p_cat_binary = {\n",
    "    \"NonTB_NonLTBI\": 0,\n",
    "    \"S+C+\": 1,\n",
    "    \"S-C+\": 1,\n",
    "    \"Clinical_TB\": 1,\n",
    "    \"Likely_subcl_TB\": 0,\n",
    "    \"NonTB_LTBI\": 0,\n",
    "}\n",
    "clinical[\"y\"] = clinical[\"p_cat\"].map(lambda x: p_cat_binary[x])\n",
    "\n",
    "\n",
    "X_med = pd.read_csv(PROCESSED_DIR / \"X_med.csv\", index_col=0)\n",
    "test_set_pred = pd.read_excel(OUTPUT_DIR / \"test_set_predictions.xlsx\", index_col=0)[\n",
    "    [\"Estimated TB Probability\", \"Predicted Diagnosis\"]\n",
    "]\n",
    "test_set_pred[\"Predicted Diagnosis\"] = test_set_pred[\"Predicted Diagnosis\"] == \"TB\"\n",
    "test_set_pred.index = test_set_pred.index.map(lambda x: Barcode2(x).any_aliquot())\n",
    "alere_dfs = [\n",
    "    pd.read_csv(\n",
    "        io,\n",
    "        index_col=0,\n",
    "        true_values=[\"Positive\", \"Positive \"],\n",
    "        false_values=[\"Negative\", \"Negative \"],\n",
    "    ).rename(\n",
    "        columns={\n",
    "            f\"Sample Barcode{col}\": \"AlereLAM Barcode\"\n",
    "            for col in [\" Number\", \" Number \", \"\"]\n",
    "        }\n",
    "        | {\"Result\": \"AlereLAM Result\", \"Notes\": \"AlereLAM Notes\"}\n",
    "    )\n",
    "    for io in os.scandir(ALERE_RESULTS_DIR)\n",
    "    if io.name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "alere = pd.concat(alere_dfs, ignore_index=True)\n",
    "alere_replacements = {\n",
    "    \"FIND 05 61 0329 U23 06\": \"FIND 05 61 0309 U23 06\",\n",
    "    \"FIND 05 61 3075 U23 01\": \"FIND 05 61 0375 U23 01\",\n",
    "    \"FIND 05 01 0094 U23 01\": \"FIND 05 01 0091 U23 01\",\n",
    "    \"FIND 05 06 0021 U23 01\": \"FIND 05 61 0021 U23 01\",\n",
    "    \"FIND 05 61 0215 U23 01\": \"FIND 05 61 0125 U23 01\",\n",
    "}\n",
    "\n",
    "alere.index = (\n",
    "    alere[\"AlereLAM Barcode\"]\n",
    "    .replace(alere_replacements)\n",
    "    .map(lambda x: Barcode2(x).any_aliquot())\n",
    ")\n",
    "\n",
    "\n",
    "def process_notes(x):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    if \"faint\" in x:\n",
    "        return \"faint line\"\n",
    "    elif \"dark\" in x:\n",
    "        return \"dark urine\"\n",
    "    else:\n",
    "        return x.strip()\n",
    "\n",
    "\n",
    "alere[\"AlereLAM Notes\"] = (\n",
    "    alere[\"AlereLAM Notes\"].map(process_notes).convert_dtypes().astype(\"category\")\n",
    ")\n",
    "clinical[\"R1 Numeric Prediction\"] = clinical[\"R1 Numeric Prediction\"].astype(\"Float64\")\n",
    "clinical = pd.concat([clinical, X_med, test_set_pred, alere], axis=1).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAM Spline models and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sens_at_spec(y_true, y_score, spec=1):\n",
    "    fpr, tpr, __ = metrics.roc_curve(y_true, y_score)\n",
    "    valid_tpr = tpr[fpr <= 1 - spec]\n",
    "    return valid_tpr.max() if valid_tpr.size else 0.0\n",
    "\n",
    "\n",
    "scorers = {\n",
    "    \"roc_auc\": metrics.get_scorer(\"roc_auc\"),\n",
    "    \"sens_at_perf_spec\": metrics.make_scorer(\n",
    "        sens_at_spec, response_method=(\"decision_function\", \"predict_proba\")\n",
    "    ),\n",
    "    \"balanced_accuracy\": metrics.get_scorer(\"balanced_accuracy\"),\n",
    "}\n",
    "\n",
    "\n",
    "y_tvt = clinical[\"y\"].loc[clinical[X_med.columns].notna().all(axis=1)]\n",
    "X_tvt = np.arcsinh(\n",
    "    clinical.loc[y_tvt.index, X_med.columns] / LOD_COL_FMT[\"LOD_samples\"] * 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run nested cross-validation (warning: long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "random_state = 0\n",
    "inner_repeats = 4\n",
    "outer_repeats = 20\n",
    "n_trials = 200\n",
    "negligible = 0.01\n",
    "\n",
    "outer_cv = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=outer_repeats, random_state=random_state\n",
    ")\n",
    "ncv_tvt = NestedCV(\n",
    "    outer_cv,\n",
    "    n_trials=n_trials,\n",
    "    inner_repeats=inner_repeats,\n",
    "    negligible=negligible,\n",
    "    n_jobs=1,\n",
    ")\n",
    "ncv_tvt.fit(X_tvt, y_tvt.values.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: load existing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PROCESSED_DIR / \"ncv_tvt.pkl\", \"rb\") as f:\n",
    "    ncv_tvt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to Excel file with clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_2 = pd.concat(\n",
    "    [\n",
    "        clinical,\n",
    "        ncv_tvt.predicted_proba.mean(axis=1).rename(\n",
    "            \"Average Predicted Probability in Nested CV\"\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "bool_cols = clinical_2.columns[clinical_2.dtypes == \"boolean\"]\n",
    "clinical_2[bool_cols] = clinical_2[bool_cols].astype(\"Int8\")\n",
    "clinical_2[\"OS_PatientID_f\"] = clinical_2[\"OS_PatientID_b\"] = clinical_2[\"OS_PatientID\"]\n",
    "clinical_2.to_excel(OUTPUT_DIR / \"clinical_combined.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_analysis = pd.read_excel(META_ANALYSIS_FILE, header=[0, 1])\n",
    "\n",
    "\n",
    "def proportion_ci(p, n, z):\n",
    "    return p + z * np.sqrt(p * (1 - p) / n)\n",
    "\n",
    "\n",
    "fill_value = {\n",
    "    (stat, col): proportion_ci(\n",
    "        meta_analysis[(stat, \"Point\")], meta_analysis[(stat, \"N\")], z\n",
    "    )\n",
    "    for stat in [\"Sensitivity\", \"Specificity\"]\n",
    "    for col, z in zip([\"Low\", \"High\"], [-1.96, 1.96])\n",
    "}\n",
    "meta_analysis.fillna(fill_value, inplace=True)\n",
    "\n",
    "\n",
    "def threshold_average_with_ci(\n",
    "    predicted_proba, y_tvt, alpha=0.05, specific_threshold=0.72\n",
    "):\n",
    "    thresh_vals = np.arange(0, 1.00001, 0.00001)\n",
    "    fpr_list = []\n",
    "    tpr_list = []\n",
    "    thresh_list = []\n",
    "    auc_list = []\n",
    "    tpr_at_thresh_list = []\n",
    "    fpr_at_thresh_list = []\n",
    "\n",
    "    for fold in predicted_proba.columns:\n",
    "        y_scores = predicted_proba[fold].dropna()\n",
    "        y_true = y_tvt.loc[y_scores.index]\n",
    "\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_true, y_scores)\n",
    "        auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        fpr_list.append(fpr)\n",
    "        tpr_list.append(tpr)\n",
    "        thresh_list.append(thresholds)\n",
    "        auc_list.append(auc)\n",
    "\n",
    "        # Compute TPR and FPR at the specific threshold\n",
    "        y_pred = (y_scores >= specific_threshold).astype(int)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
    "        tpr_at_thresh = tp / (tp + fn)\n",
    "        fpr_at_thresh = fp / (fp + tn)\n",
    "        tpr_at_thresh_list.append(tpr_at_thresh)\n",
    "        fpr_at_thresh_list.append(fpr_at_thresh)\n",
    "\n",
    "    pr_d = {\"fpr\": [], \"tpr\": []}\n",
    "\n",
    "    for value, key_l in zip(pr_d.values(), [fpr_list, tpr_list]):\n",
    "        for thresholds_fold, l_fold in zip(thresh_list, key_l):\n",
    "            # Reverse the thresholds to ensure they are increasing\n",
    "            thresholds_fold_reversed = thresholds_fold[::-1]\n",
    "            fold_reversed = l_fold[::-1]\n",
    "            interp = np.interp(thresh_vals, thresholds_fold_reversed, fold_reversed)\n",
    "            value.append(interp)\n",
    "\n",
    "    n_folds = len(fpr_list)\n",
    "    mean_d = {k: np.mean(v, axis=0) for k, v in pr_d.items()}\n",
    "    se_d = {k: np.std(v, axis=0, ddof=1) / np.sqrt(n_folds) for k, v in pr_d.items()}\n",
    "\n",
    "    # Compute standard errors and confidence intervals\n",
    "    t_crit = stats.t.ppf(1 - alpha / 2, df=n_folds - 1)\n",
    "    ci_upper = {k: mean_d[k] + t_crit * se_d[k] for k in mean_d}\n",
    "    ci_lower = {k: mean_d[k] - t_crit * se_d[k] for k in mean_d}\n",
    "\n",
    "    # Compute mean and confidence intervals for AUC\n",
    "    avg_auc = np.mean(auc_list)\n",
    "    se_auc = np.std(auc_list, ddof=1) / np.sqrt(n_folds)\n",
    "    ci_upper_auc = avg_auc + t_crit * se_auc\n",
    "    ci_lower_auc = avg_auc - t_crit * se_auc\n",
    "\n",
    "    # Compute mean and confidence intervals for TPR and FPR at the specific threshold\n",
    "    mean_tpr_at_thresh = np.mean(tpr_at_thresh_list)\n",
    "    se_tpr_at_thresh = np.std(tpr_at_thresh_list, ddof=1) / np.sqrt(n_folds)\n",
    "    ci_lower_tpr_at_thresh = mean_tpr_at_thresh - t_crit * se_tpr_at_thresh\n",
    "    ci_upper_tpr_at_thresh = mean_tpr_at_thresh + t_crit * se_tpr_at_thresh\n",
    "\n",
    "    mean_fpr_at_thresh = np.mean(fpr_at_thresh_list)\n",
    "    se_fpr_at_thresh = np.std(fpr_at_thresh_list, ddof=1) / np.sqrt(n_folds)\n",
    "    ci_lower_fpr_at_thresh = mean_fpr_at_thresh - t_crit * se_fpr_at_thresh\n",
    "    ci_upper_fpr_at_thresh = mean_fpr_at_thresh + t_crit * se_fpr_at_thresh\n",
    "\n",
    "    result = {\n",
    "        \"roc_curve\": {\n",
    "            \"mean_fpr\": mean_d[\"fpr\"],\n",
    "            \"mean_tpr\": mean_d[\"tpr\"],\n",
    "            \"ci_fpr\": {\"lower\": ci_lower[\"fpr\"], \"upper\": ci_upper[\"fpr\"]},\n",
    "            \"ci_tpr\": {\"lower\": ci_lower[\"tpr\"], \"upper\": ci_upper[\"tpr\"]},\n",
    "            \"thresholds\": thresh_vals,\n",
    "        },\n",
    "        \"auc\": {\n",
    "            \"mean\": avg_auc,\n",
    "            \"ci_lower\": ci_lower_auc,\n",
    "            \"ci_upper\": ci_upper_auc,\n",
    "        },\n",
    "        \"tpr_at_threshold\": {\n",
    "            \"threshold\": specific_threshold,\n",
    "            \"mean\": mean_tpr_at_thresh,\n",
    "            \"ci_lower\": ci_lower_tpr_at_thresh,\n",
    "            \"ci_upper\": ci_upper_tpr_at_thresh,\n",
    "        },\n",
    "        \"fpr_at_threshold\": {\n",
    "            \"threshold\": specific_threshold,\n",
    "            \"mean\": mean_fpr_at_thresh,\n",
    "            \"ci_lower\": ci_lower_fpr_at_thresh,\n",
    "            \"ci_upper\": ci_upper_fpr_at_thresh,\n",
    "        },\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "overall = pd.Series(data=True, index=y_tvt.index)\n",
    "hiv_status = {\n",
    "    \"any HIV\": overall,\n",
    "    \"HIVâ€“\": ~clinical.loc[y_tvt.index, \"HIV_status\"],\n",
    "    \"HIV+\": clinical.loc[y_tvt.index, \"HIV_status\"],\n",
    "}\n",
    "smear_status = {\n",
    "    \"overall\": overall,\n",
    "    \"Smear+\": ~clinical.loc[y_tvt.index, \"p_cat\"].isin([\"S-C+\", \"Clinical_TB\"]),\n",
    "    \"Smearâ€“\": clinical.loc[y_tvt.index, \"p_cat\"] != \"S+C+\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries for New Figure 2\n",
    "\n",
    "color_marker = {\n",
    "    \"TPP\": \"#491E5D\",\n",
    "    \"AlereLAM\": \"#D98825\",\n",
    "    # \"Truenat MTB\": (\"indianred\", \"o\"),\n",
    "    \"Truenat Plus\": \"#fe0809\",\n",
    "    # \"Xpert MTB/RIF\": (\"#3377BB\", \"o\"),\n",
    "    \"Xpert Ultra\": \"#0077c8\",\n",
    "    # \"EclLAM\": (\"#009E73\", \"o\"),\n",
    "    \"Smear microscopy\": \"#CC79A7\",\n",
    "    # \"Smear (ZN)\": (\"brown\", \"o\"),\n",
    "    # \"Smear (FM)\": (\"brown\", \"o\"),\n",
    "    \"Culture\": \"brown\",\n",
    "}\n",
    "\n",
    "masks = {\n",
    "    \"All\": pd.Series(data=True, index=y_tvt.index),\n",
    "    \"HIV-\": ~clinical.loc[y_tvt.index, \"HIV_status\"],\n",
    "    \"HIV+\": clinical.loc[y_tvt.index, \"HIV_status\"],\n",
    "    \"S+\": ~clinical.loc[y_tvt.index, \"p_cat\"].isin([\"S-C+\", \"Clinical_TB\"]),\n",
    "    \"S-\": clinical.loc[y_tvt.index, \"p_cat\"] != \"S+C+\",\n",
    "}\n",
    "\n",
    "tpp_sizes = {\n",
    "    \"Optimal\": (0.95, 0.05, 1),\n",
    "    \"Low-complexity\": (0.8, 0.15, 0.6),\n",
    "    \"Near point of care\": (0.75, 0.05, 1 / 3),\n",
    "    \"Point of care\": (0.65, 0.1, 0.2),\n",
    "}\n",
    "\n",
    "comparison_zorder = {\n",
    "    \"Xpert Ultra\": -1,\n",
    "    \"AlereLAM\": -2,\n",
    "    \"Truenat Plus\": -3,\n",
    "    \"Smear microscopy\": -4,\n",
    "}\n",
    "\n",
    "\n",
    "def add_errorbar(row):\n",
    "    x = 1 - row[(\"Specificity\", \"Point\")]\n",
    "    y = row[(\"Sensitivity\", \"Point\")]\n",
    "    yerr = np.array(\n",
    "        [row[(\"Sensitivity\", \"High\")] - y, y - row[(\"Sensitivity\", \"Low\")]]\n",
    "    ).reshape((2, 1))\n",
    "    xerr = np.array(\n",
    "        [1 - row[(\"Specificity\", \"Low\")] - x, x - 1 + row[(\"Specificity\", \"High\")]]\n",
    "    ).reshape((2, 1))\n",
    "    label = row[(\"Condition\", \"Assay\")]\n",
    "    ms_spec_n = row[(\"Specificity\", \"N\")]\n",
    "    if ms_spec_n == 0:\n",
    "        ms_spec_n = 2858\n",
    "    markersize = (row[(\"Sensitivity\", \"N\")] * ms_spec_n) ** 0.5 / 200\n",
    "    if markersize > 16:\n",
    "        markersize = 16\n",
    "    return dict(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        yerr=yerr,\n",
    "        xerr=xerr,\n",
    "        marker=\"o\",\n",
    "        color=color_marker[label],\n",
    "        label=label,\n",
    "        markersize=markersize,\n",
    "        zorder=comparison_zorder[label],\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_thresh_ci(predicted_proba, y_tvt, alpha=0.05, specific_threshold=0.72):\n",
    "    result = threshold_average_with_ci(\n",
    "        predicted_proba, y_tvt, alpha=alpha, specific_threshold=specific_threshold\n",
    "    )\n",
    "\n",
    "    # Extract ROC curve data\n",
    "    roc_curve = result[\"roc_curve\"]\n",
    "    fpr = roc_curve[\"mean_fpr\"]\n",
    "    tpr = roc_curve[\"mean_tpr\"]\n",
    "    fpr_ci_lower = roc_curve[\"ci_fpr\"][\"lower\"]\n",
    "    fpr_ci_upper = roc_curve[\"ci_fpr\"][\"upper\"]\n",
    "    tpr_ci_lower = roc_curve[\"ci_tpr\"][\"lower\"]\n",
    "    tpr_ci_upper = roc_curve[\"ci_tpr\"][\"upper\"]\n",
    "\n",
    "    # Extract AUC data\n",
    "    auc_data = result[\"auc\"]\n",
    "    roc_auc = auc_data[\"mean\"]\n",
    "    auc_ci_lower = auc_data[\"ci_lower\"]\n",
    "    auc_ci_upper = auc_data[\"ci_upper\"]\n",
    "\n",
    "    # Plot the mean ROC curve\n",
    "    roc_mean = (fpr, tpr)\n",
    "\n",
    "    # Fill between the confidence intervals for TPR\n",
    "    roc_ci = {\"x\": fpr, \"y1\": tpr_ci_lower, \"y2\": tpr_ci_upper}\n",
    "\n",
    "    # Plot TPR and FPR at the specific threshold\n",
    "    tpr_at_thresh = result[\"tpr_at_threshold\"][\"mean\"]\n",
    "    tpr_at_thresh_ci_lower = result[\"tpr_at_threshold\"][\"ci_lower\"]\n",
    "    tpr_at_thresh_ci_upper = result[\"tpr_at_threshold\"][\"ci_upper\"]\n",
    "\n",
    "    fpr_at_thresh = result[\"fpr_at_threshold\"][\"mean\"]\n",
    "    fpr_at_thresh_ci_lower = result[\"fpr_at_threshold\"][\"ci_lower\"]\n",
    "    fpr_at_thresh_ci_upper = result[\"fpr_at_threshold\"][\"ci_upper\"]\n",
    "\n",
    "    # Plot the point at the specific threshold with error bars\n",
    "    errorbar = {\n",
    "        \"x\": fpr_at_thresh,\n",
    "        \"y\": tpr_at_thresh,\n",
    "        \"xerr\": [\n",
    "            [fpr_at_thresh - fpr_at_thresh_ci_lower],\n",
    "            [fpr_at_thresh_ci_upper - fpr_at_thresh],\n",
    "        ],\n",
    "        \"yerr\": [\n",
    "            [tpr_at_thresh - tpr_at_thresh_ci_lower],\n",
    "            [tpr_at_thresh_ci_upper - tpr_at_thresh],\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Uncomment to fill between FPR confidence intervals\n",
    "    # ax.fill_betweenx(tpr, fpr_ci_lower, fpr_ci_upper, alpha=0.2, color=\"C1\")\n",
    "    return roc_mean, roc_ci, errorbar, auc_data\n",
    "\n",
    "\n",
    "title_weight = \"semibold\"\n",
    "legend_kws = {\n",
    "    \"handlelength\": 1.2,\n",
    "    \"alignment\": \"left\",\n",
    "    \"fancybox\": False,\n",
    "    \"title_fontproperties\": {\"weight\": title_weight},\n",
    "    \"framealpha\": 0.5,\n",
    "}\n",
    "\n",
    "panel_titles = {\n",
    "    \"All\": (\"All Samples (N=576)\", \"A\"),\n",
    "    \"S+\": (\"Smear+\", \"B\"),\n",
    "    \"S-\": (\"Smearâ€“\", \"C\"),\n",
    "    \"HIV-\": (\"HIVâ€“\", \"D\"),\n",
    "    \"HIV+\": (\"HIV+\", \"E\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axd = plt.subplot_mosaic(\n",
    "    [[\"All\", \"All\", \"S+\"], [\"All\", \"All\", \"S-\"], [\"HIV-\", \"HIV+\", \"Legend\"]],\n",
    "    figsize=(10, 10),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "legend_handles = {key: [] for key in masks}\n",
    "legend_legends = {\n",
    "    \"Nested cross-validation\": [],\n",
    "    \"Comparisons from literature\": [],\n",
    "    \"WHO target product profile\": [],\n",
    "}\n",
    "\n",
    "for a, mask in masks.items():\n",
    "    roc_mean, roc_ci, errorbar, auc_data = plot_thresh_ci(\n",
    "        ncv_tvt.predicted_proba.loc[mask], y_tvt.loc[mask]\n",
    "    )\n",
    "    axd[a].plot(\n",
    "        *roc_mean, color=\"k\", solid_joinstyle=\"miter\", solid_capstyle=\"projecting\"\n",
    "    )\n",
    "    legend_handles[a].append(\n",
    "        axd[a].plot([], [], \"s-\", color=\"k\", label=f\" AUC = {auc_data['mean']:.3f}\")[0]\n",
    "    )\n",
    "    legend_handles[a].append(\n",
    "        axd[a].fill_between(\n",
    "            **roc_ci,\n",
    "            color=\"k\",\n",
    "            alpha=0.2,\n",
    "            label=f\"({auc_data['ci_lower']:.3f}â€“{auc_data['ci_upper']:.3f})\",\n",
    "            lw=0,\n",
    "        )\n",
    "    )\n",
    "    axd[a].errorbar(**errorbar, fmt=\"s\", color=\"k\")\n",
    "    axd[a].set(\n",
    "        xlim=(0, 1),\n",
    "        ylim=(0, 1),\n",
    "        xlabel=\"1â€“Specificity (False Positive Rate)\",\n",
    "        ylabel=\"Sensitivity (True Positive Rate)\",\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "    axd[a].set_xticks(np.arange(0, 1.01, 0.02), minor=True)\n",
    "    axd[a].set_yticks(np.arange(0, 1.01, 0.02), minor=True)\n",
    "    axd[a].set_xticks(np.arange(0, 1.01, 0.1))\n",
    "    axd[a].set_yticks(np.arange(0, 1.01, 0.1))\n",
    "    if a == \"All\":\n",
    "        labels = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    else:\n",
    "        labels = [0, \"\", 0.2, \"\", 0.4, \"\", 0.6, \"\", 0.8, \"\", 1]\n",
    "    axd[a].set_xticklabels(labels)\n",
    "    axd[a].set_yticklabels(labels)\n",
    "\n",
    "# Test Set\n",
    "data = clinical.loc[y_tvt.index].dropna(subset=\"Estimated TB Probability\")\n",
    "# Test set ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(data[\"y\"], data[\"Estimated TB Probability\"])\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "axd[\"S-\"].plot(\n",
    "    fpr,\n",
    "    tpr,\n",
    "    color=\"#00664A\",\n",
    "    solid_joinstyle=\"miter\",\n",
    "    solid_capstyle=\"projecting\",\n",
    ")\n",
    "fpr = data[\"Predicted Diagnosis\"][data[\"y\"] == 0].mean()\n",
    "tpr = data[\"Predicted Diagnosis\"][data[\"y\"] == 1].mean()\n",
    "legend_handles[\"S-\"].append(\n",
    "    axd[\"S-\"].plot(\n",
    "        fpr, tpr, \"s-\", label=f\"Blinded test\\nset, AUC={auc:.3f}\", color=\"#00664A\"\n",
    "    )[0]\n",
    ")\n",
    "\n",
    "# AlereLAM on test set\n",
    "tpr = data[\"AlereLAM Result\"][data[\"y\"] == 1].mean()\n",
    "fpr = data[\"AlereLAM Result\"][data[\"y\"] == 0].mean()\n",
    "legend_handles[\"S-\"].append(\n",
    "    axd[\"S-\"].plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        \"D\",\n",
    "        label=\"AlereLAM in\\nblinded test set\",\n",
    "        color=color_marker[\"AlereLAM\"],\n",
    "        zorder=-1,\n",
    "    )[0]\n",
    ")\n",
    "\n",
    "# TPP\n",
    "for label, (loc, height, alpha) in tpp_sizes.items():\n",
    "    legend_legends[\"WHO target product profile\"].append(\n",
    "        axd[\"All\"].add_patch(\n",
    "            matplotlib.patches.Rectangle(\n",
    "                (0, loc),\n",
    "                width=0.02,\n",
    "                height=height,\n",
    "                color=color_marker[\"TPP\"],\n",
    "                alpha=alpha,\n",
    "                lw=0,\n",
    "                label=label,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Comparison assays\n",
    "for row in meta_analysis[meta_analysis[(\"Total\", \"axd\")].notna()].iterrows():\n",
    "    kwargs = add_errorbar(row[1])\n",
    "    axs = row[1][(\"Total\", \"axd\")].split(\", \")\n",
    "    if row[1][(\"Total\", \"legend\")] == 1:\n",
    "        axs.append(\"Legend\")\n",
    "    for a in axs:\n",
    "        eb = axd[a].errorbar(**kwargs)\n",
    "        if a == \"Legend\":\n",
    "            legend_legends[\"Comparisons from literature\"].append(eb)\n",
    "\n",
    "for a in [\"All\", \"HIV-\", \"HIV+\"]:\n",
    "    empirical_spos = (\n",
    "        clinical.loc[y_tvt.index][\"p_cat\"][masks[a] & y_tvt] == \"S+C+\"\n",
    "    ).sum() / (masks[a] & y_tvt).sum()\n",
    "    (artist,) = axd[a].plot(\n",
    "        0.02,\n",
    "        empirical_spos,\n",
    "        \"D\",\n",
    "        color=color_marker[\"Smear microscopy\"],\n",
    "        zorder=-3,\n",
    "        label=\"Smear positivity rate in cohort\",\n",
    "    )\n",
    "legend_legends[\"Comparisons from literature\"].append(artist)\n",
    "\n",
    "legend_legends[\"Nested cross-validation\"] = [\n",
    "    axd[\"Legend\"].plot([], [], \"s-\", color=\"k\", label=\"Mean (area under curve, AUC)\")[\n",
    "        0\n",
    "    ],\n",
    "    axd[\"Legend\"].fill_between(\n",
    "        [], [], [], color=\"k\", alpha=0.2, label=\"95% confidence interval\", lw=0\n",
    "    ),\n",
    "]\n",
    "\n",
    "locs = [(0, 0.74), (0, 0.29), (0, -0.09)]\n",
    "for (title, handles), loc in zip(legend_legends.items(), locs):\n",
    "    artist = axd[\"Legend\"].legend(\n",
    "        handles=handles, title=title, loc=loc, mode=\"expand\", **legend_kws\n",
    "    )\n",
    "    axd[\"Legend\"].add_artist(artist)\n",
    "\n",
    "\n",
    "# Legends\n",
    "axd[\"Legend\"].axis(\"off\")\n",
    "axd[\"Legend\"].set(xlim=(-2, -1), ylim=(-2, -1))\n",
    "for a in masks:\n",
    "    axd[a].legend(handles=legend_handles[a], loc=\"lower right\", **legend_kws)\n",
    "\n",
    "# Titles\n",
    "for a, (title, l) in panel_titles.items():\n",
    "    dy = 0.01\n",
    "    dx = 0.09\n",
    "    y = 1 - dy if title.startswith(\"All\") else 1 - 2 * dy\n",
    "    xl = dx if title.startswith(\"All\") else 2 * dx\n",
    "    axd[a].text(x=0.5, y=y, s=title, ha=\"center\", va=\"top\", weight=title_weight)\n",
    "    axd[a].text(x=xl, y=y, s=f\"({l})\", ha=\"left\", va=\"top\", weight=title_weight)\n",
    "\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / \"f2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of sensitivity and specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_s_cols = pd.MultiIndex.from_product(\n",
    "    [\n",
    "        [\"auc\", \"tpr_at_threshold\", \"fpr_at_threshold\", \"sensitivity\", \"specificity\"],\n",
    "        [\"mean\", \"ci_lower\", \"ci_upper\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "sens_spec = pd.DataFrame(\n",
    "    index=pd.MultiIndex.from_product([hiv_status, smear_status]),\n",
    "    columns=s_s_cols,\n",
    ")\n",
    "\n",
    "for hiv_label, hiv_selector in hiv_status.items():\n",
    "    for smear_label, smear_selector in smear_status.items():\n",
    "        mask = hiv_selector & smear_selector\n",
    "        y_true = y_tvt.loc[mask]\n",
    "        d = threshold_average_with_ci(ncv_tvt.predicted_proba.loc[mask], y_true)\n",
    "        for score, stat in s_s_cols:\n",
    "            if score in d:\n",
    "                sens_spec.loc[(hiv_label, smear_label), (score, stat)] = d[score][stat]\n",
    "\n",
    "sens_spec[\"sensitivity\"] = sens_spec[\"tpr_at_threshold\"]\n",
    "sens_spec[\"specificity\"] = 1 - sens_spec[\"fpr_at_threshold\"]\n",
    "sens_spec = sens_spec.drop([\"tpr_at_threshold\", \"fpr_at_threshold\"], axis=1, level=0)\n",
    "sens_spec.to_excel(OUTPUT_DIR / \"sens_spec_table.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1\n",
    "countries = clinical[\"Country\"].value_counts().index\n",
    "\n",
    "tb_categories = {\"S+C+\": \"S+C+\", \"S-C+\": \"S-C+\", \"Clinical_TB\": \"Clinically diagnosed\"}\n",
    "cont_cats = {\n",
    "    (\"HIV+\", \"CD4 count*\\n(cells/Î¼L)\"): \"HIV_CD4CNT\",\n",
    "    (\"Age\\n(years)\",): \"Age at Enrol\",\n",
    "}\n",
    "table_index = pd.MultiIndex.from_tuples(\n",
    "    [(\"N\", \"Initial\"), (\"N\", \"Excluded\"), (\"N\", \"Measured\"), (\"Male\",), (\"HIV+\",)]\n",
    "    + list(cont_cats)\n",
    "    + [(\"Country\", country) for country in countries]\n",
    "    + [(\"TB\",)]\n",
    "    + [(\"TB\", cat) for cat in tb_categories.values()]\n",
    "    + [(\"Non-TB\",), (\"Non-TB\", \"Latent TB**\")]\n",
    ")\n",
    "\n",
    "table_1_cols = list(\n",
    "    clinical[\"Cohort\"].unique().map(lambda s: s.title(), na_action=None)\n",
    ") + [\"All\"]\n",
    "table_1 = pd.DataFrame(columns=table_1_cols, index=table_index)\n",
    "\n",
    "for cohort in table_1.columns:\n",
    "    df = clinical if cohort == \"All\" else clinical[clinical[\"Cohort\"] == cohort.lower()]\n",
    "    n = df.shape[0]\n",
    "    table_1.loc[(\"N\", \"Initial\"), cohort] = n\n",
    "\n",
    "    excluded = df[\"488 Ag85B 182\"].isna().sum()\n",
    "    table_1.loc[(\"N\", \"Excluded\"), cohort] = f\"{excluded}\\n({excluded / n:.0%})\"\n",
    "    df = df[df[\"488 Ag85B 182\"].notna()]\n",
    "\n",
    "    measured = df.shape[0]\n",
    "    table_1.loc[(\"N\", \"Measured\"), cohort] = f\"{measured}\\n({measured / n:.0%})\"\n",
    "\n",
    "    male = (df[\"SEX\"] == \"Male\").sum()\n",
    "    table_1.loc[\"Male\", cohort] = f\"{male}\\n({male / measured:.0%})\"\n",
    "\n",
    "    hiv = df[\"HIV_status\"].sum()\n",
    "    table_1.loc[\"HIV+\", cohort] = f\"{hiv}\\n({hiv / measured:.0%})\"\n",
    "\n",
    "    for r, df_col in cont_cats.items():\n",
    "        col = df[df_col][df[df_col] > 0]\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            table_1.loc[r, cohort] = f\"{round(col.median())}\\n({col.min()}â€“{col.max()})\"\n",
    "\n",
    "    for country in countries:\n",
    "        c = (df[\"Country\"] == country).sum()\n",
    "        table_1.loc[(\"Country\", country), cohort] = f\"{c}\\n({c / measured:.0%})\"\n",
    "\n",
    "    tb = df[\"p_cat\"].isin(tb_categories).sum()\n",
    "    table_1.loc[\"TB\", cohort] = f\"{tb}\\n({tb / measured:.0%})\"\n",
    "    for p_cat, row in tb_categories.items():\n",
    "        n_p_cat = (df[\"p_cat\"] == p_cat).sum()\n",
    "        table_1.loc[(\"TB\", row), cohort] = f\"{n_p_cat}\\n({n_p_cat / tb:.0%})\"\n",
    "\n",
    "    ntb = df[\"p_cat\"].isin([\"NonTB_LTBI\", \"NonTB_NonLTBI\", \"Likely_subcl_TB\"]).sum()\n",
    "    table_1.loc[\"Non-TB\", cohort] = f\"{ntb}\\n({ntb / measured:.0%})\"\n",
    "\n",
    "    latent_tb = (\n",
    "        (df[\"p_cat\"] == \"NonTB_LTBI\")\n",
    "        | ((df[\"p_cat\"] == \"Likely_subcl_TB\") & df[\"QFT_RES\"])\n",
    "    ).sum()\n",
    "    table_1.loc[(\"Non-TB\", \"Latent TB**\"), cohort] = (\n",
    "        f\"{latent_tb}\\n({latent_tb / ntb:.0%})\"\n",
    "    )\n",
    "\n",
    "writer = pd.ExcelWriter(OUTPUT_DIR / \"table_1.xlsx\", engine=\"xlsxwriter\")\n",
    "table_1.to_excel(writer, sheet_name=\"Sheet1\")\n",
    "cell_format = writer.book.add_format(\n",
    "    {\"font_name\": \"Times New Roman\", \"font_size\": 12, \"align\": \"center\", \"border\": 1}\n",
    ")\n",
    "writer.sheets[\"Sheet1\"].set_column(0, len(table_1.columns) + 1, None, cell_format)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_ticks(divisor, ticks, labels=None):\n",
    "    trans_ticks = np.arcsinh(np.asarray(ticks) / divisor)\n",
    "    if labels is None:\n",
    "        labels = ticks\n",
    "    return {\"ticks\": trans_ticks, \"labels\": labels}\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(9, 4.5), width_ratios=[1, 3])\n",
    "\n",
    "ag85b_div = 0.04\n",
    "lam_div = 4\n",
    "X_trans = np.arcsinh(\n",
    "    clinical.loc[y_tvt.index, X_med.columns]\n",
    "    / pd.Series(index=X_med.columns, data=[ag85b_div] + [lam_div] * 3)\n",
    ")\n",
    "\n",
    "tb_colors = {False: \"#192666\", True: \"#993300\"}\n",
    "tb_labels = [\"Non-TB\", \"TB\"]\n",
    "\n",
    "swarm_df = pd.concat(\n",
    "    [clinical.loc[X_trans.index, [\"p_cat\", \"HIV_status\", \"y\"]], X_trans], axis=1\n",
    ").melt(id_vars=[\"HIV_status\", \"y\", \"p_cat\"], var_name=\"Plex\")\n",
    "swarm_df[\"x\"] = swarm_df.apply(lambda row: f\"{row.Plex}\\n{row.y}\", axis=1)\n",
    "\n",
    "kwargs = {\"size\": 2, \"x\": \"x\", \"y\": \"value\"}\n",
    "ylim = (swarm_df[\"value\"].min() * 1.01, swarm_df[\"value\"].max() * 1.01)\n",
    "sns.swarmplot(swarm_df[swarm_df[\"Plex\"] == \"488 Ag85B 182\"], ax=axs[0], **kwargs)\n",
    "axs[0].set_xticks([0, 1], labels=[\"Non-TB\\nAg85B\", \"TB\\nAg85B\"])\n",
    "axs[0].set_yticks(**transform_ticks(ag85b_div, [0, 0.1, 1, 10, 100]))\n",
    "axs[0].set(ylabel=\"Ag85B (pg/mL)\", ylim=ylim, xlabel=\"\")\n",
    "\n",
    "sns.swarmplot(swarm_df[swarm_df[\"Plex\"] != \"488 Ag85B 182\"], ax=axs[1], **kwargs)\n",
    "axs[1].set_xticks(\n",
    "    np.arange(6),\n",
    "    labels=[\n",
    "        f\"{p_cat}\\n{plex.split()[-1]}\"\n",
    "        for plex in X_trans.columns[1:]\n",
    "        for p_cat in [\"Non-TB\", \"TB\"]\n",
    "    ],\n",
    ")\n",
    "axs[1].set_yticks(**transform_ticks(lam_div, [0, 10, 100, 1000, 10000]))\n",
    "axs[1].set(ylabel=\"LAM (pg/mL)\", ylim=ylim, xlabel=\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"f3.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_var_type(s):\n",
    "    if s.nunique() == 2:\n",
    "        return \"binary\"\n",
    "    elif pd.api.types.is_numeric_dtype(s):\n",
    "        return \"numeric\"\n",
    "    elif isinstance(s.dtype, pd.CategoricalDtype):\n",
    "        return \"categorical\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data type {s.dtype} in column {s.name}\")\n",
    "\n",
    "\n",
    "def group(num_data, cat_data):\n",
    "    return [num_data[cat_data == val] for val in cat_data.unique()]\n",
    "\n",
    "\n",
    "def _choose_mwu_method(n1, n2, has_ties):\n",
    "    \"\"\"Choose appropriate Mann-Whitney U method to avoid overflow.\"\"\"\n",
    "    if (n1 > 8 and n2 > 8) or has_ties or (n1 + n2) > 20:\n",
    "        return \"asymptotic\"\n",
    "    return \"exact\"\n",
    "\n",
    "\n",
    "def mannwhitneyu_ab(num_data, bin_data):\n",
    "    groups = group(num_data, bin_data)\n",
    "    n1, n2 = len(groups[0]), len(groups[1])\n",
    "\n",
    "    # Check for ties\n",
    "    combined = np.concatenate([groups[0], groups[1]])\n",
    "    has_ties = len(combined) != len(np.unique(combined))\n",
    "\n",
    "    # Choose method dynamically\n",
    "    method = _choose_mwu_method(n1, n2, has_ties)\n",
    "\n",
    "    u, p_value = stats.mannwhitneyu(\n",
    "        groups[0], groups[1], alternative=\"two-sided\", method=method\n",
    "    )\n",
    "    rrb = 1 - (2 * u) / (n1 * n2)\n",
    "    return rrb, p_value\n",
    "\n",
    "\n",
    "def mannwhitneyu_ba(bin_data, num_data):\n",
    "    return mannwhitneyu_ab(num_data, bin_data)\n",
    "\n",
    "\n",
    "def kruskal_ab(num_data, cat_data):\n",
    "    groups = group(num_data, cat_data)\n",
    "\n",
    "    # Check for valid groups (each group must have at least 1 observation)\n",
    "    groups = [g for g in groups if len(g) > 0]\n",
    "    if len(groups) < 2:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    h, p_value = stats.kruskal(*groups)\n",
    "\n",
    "    # Fixed eta-squared calculation\n",
    "    n_total = sum(len(g) for g in groups)\n",
    "    k = len(groups)\n",
    "\n",
    "    if n_total == k:  # Each group has exactly 1 observation\n",
    "        return np.nan, p_value\n",
    "\n",
    "    # Corrected eta-squared formula for stats.kruskal-Wallis\n",
    "    eta_sq = (h - k + 1) / (n_total - k)\n",
    "    eta_sq = max(0, eta_sq)  # Ensure non-negative\n",
    "\n",
    "    return np.sqrt(eta_sq), p_value\n",
    "\n",
    "\n",
    "def kruskal_ba(cat_data, num_data):\n",
    "    return kruskal_ab(num_data, cat_data)\n",
    "\n",
    "\n",
    "def chi2(data_a, data_b):\n",
    "    contingency_table = pd.crosstab(data_a, data_b)\n",
    "\n",
    "    # Check for valid contingency table\n",
    "    if contingency_table.size == 0 or contingency_table.sum().sum() == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # Check minimum expected frequencies (common rule: all >= 5)\n",
    "    expected = (\n",
    "        contingency_table.sum(axis=0).values[:, None]\n",
    "        @ contingency_table.sum(axis=1).values[None, :]\n",
    "    )\n",
    "    expected = expected / contingency_table.sum().sum()\n",
    "\n",
    "    if (expected < 5).any():\n",
    "        # Consider using Fisher's exact test for 2x2 tables or warning for larger tables\n",
    "        pass  # Continue with chi-square but be aware of potential issues\n",
    "\n",
    "    x2, p_value, __, __ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Use CramÃ©r's V (more appropriate than phi for non-2x2 tables)\n",
    "    n = len(data_a)\n",
    "    min_dim = min(contingency_table.shape) - 1\n",
    "    cramers_v = np.sqrt(x2 / (n * min_dim)) if min_dim > 0 else 0\n",
    "\n",
    "    return cramers_v, p_value\n",
    "\n",
    "\n",
    "stat_tests = {\n",
    "    (\"numeric\", \"numeric\"): stats.spearmanr,\n",
    "    (\"numeric\", \"binary\"): mannwhitneyu_ab,\n",
    "    (\"numeric\", \"categorical\"): kruskal_ab,\n",
    "    (\"binary\", \"numeric\"): mannwhitneyu_ba,\n",
    "    (\"categorical\", \"numeric\"): kruskal_ba,\n",
    "    (\"categorical\", \"categorical\"): chi2,\n",
    "    (\"binary\", \"binary\"): chi2,\n",
    "    (\"binary\", \"categorical\"): chi2,\n",
    "    (\"categorical\", \"binary\"): chi2,\n",
    "}\n",
    "\n",
    "# Combine all columns to consider\n",
    "clinical_hyp = clinical.select_dtypes(exclude=[\"object\", \"string\"]).copy()\n",
    "clinical_hyp = clinical_hyp[clinical_hyp.columns[clinical_hyp.nunique() > 1]]\n",
    "dt_cols = clinical_hyp.select_dtypes(\"datetime64\").columns\n",
    "clinical_hyp[dt_cols] = (\n",
    "    clinical_hyp[dt_cols]\n",
    "    .map(lambda x: x.toordinal() if pd.notna(x) else pd.NA)\n",
    "    .convert_dtypes()\n",
    ")\n",
    "\n",
    "# Step 2: Iterate Over All Unique Pairs of Columns\n",
    "results = []\n",
    "\n",
    "for col_a, col_b in tqdm(list(combinations(clinical_hyp.columns, 2))):\n",
    "    # Step 3: Select and Perform Appropriate Test\n",
    "    # Prepare data by dropping NA\n",
    "    data = clinical_hyp[[col_a, col_b]].dropna()\n",
    "\n",
    "    # Check both columns have variation after dropping NAs\n",
    "    if all(data.nunique() > 1) and len(data) > 1:  # Need at least 2 observations\n",
    "        var_type_a = get_var_type(data[col_a])\n",
    "        var_type_b = get_var_type(data[col_b])\n",
    "\n",
    "        appropriate_test = stat_tests[(var_type_a, var_type_b)]\n",
    "\n",
    "        try:\n",
    "            stat, p_value = appropriate_test(data[col_a], data[col_b])\n",
    "\n",
    "            # Handle invalid results\n",
    "            if pd.isna(stat) or pd.isna(p_value):\n",
    "                continue\n",
    "\n",
    "            # Step 4: Store the Results\n",
    "            log_p_value = -np.log10(p_value) if p_value > 0 else np.nan\n",
    "            results.append(\n",
    "                {\n",
    "                    \"col_a\": col_a,\n",
    "                    \"col_b\": col_b,\n",
    "                    \"test\": appropriate_test.__name__,\n",
    "                    \"r2\": stat**2,\n",
    "                    \"p_value\": p_value,\n",
    "                    \"log_p_value\": log_p_value,\n",
    "                    \"n\": len(data),\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {col_a} vs {col_b}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# Step 5: Compile Results into a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Step 6: Adjust p-values for Multiple Comparisons\n",
    "p_values = results_df[\"p_value\"].values\n",
    "results_df[\"adjusted_p_value\"] = stats.false_discovery_control(p_values, method=\"bh\")\n",
    "results_df = results_df.convert_dtypes().sort_values(\n",
    "    [\"p_value\", \"log_p_value\", \"adjusted_p_value\", \"r2\", \"n\"],\n",
    "    ascending=[True, False, True, False, False],\n",
    ")\n",
    "\n",
    "results_df.to_excel(OUTPUT_DIR / \"correlations.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
